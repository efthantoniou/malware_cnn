import tensorflow as tf
import pathlib
import numpy as np
import os
from tensorflow.keras import layers
import argparse
import sys
import matplotlib.pyplot as plt


# tf.compat.v1.enable_eager_execution()

IMG_HEIGHT = IMG_WIDTH = 256
AUTOTUNE = tf.data.experimental.AUTOTUNE

batch_size = 64

device_name = tf.test.gpu_device_name()
if not device_name:
    raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

model_dir = os.getcwd()+'/model/'
if not os.path.exists(model_dir):
    os.makedirs(model_dir)

def get_uncompiled_model(class_names):
    model = tf.keras.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(len(class_names), activation='softmax')
    ])
    return model

def get_compiled_model(class_names):
    model = get_uncompiled_model(class_names)
    model.compile(
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
        optimizer='adam',
        metrics=['accuracy']
    )
    return model




if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('-s', '--source', type=str, help='Path to data')
    args = parser.parse_args()

    if args.source:
        pass
    else:
        sys.exit(0)

    data_dir = args.source
    data_dir = pathlib.Path(data_dir)

    image_count = len(list(data_dir.glob('*/*.png')))
    print(image_count)
    
    train_ds = tf.keras.preprocessing.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="training",
        seed=1337,
        image_size=(IMG_HEIGHT, IMG_WIDTH),
        batch_size=batch_size
    )
    
    val_ds = tf.keras.preprocessing.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="validation",
        seed=1337,
        image_size=(IMG_HEIGHT, IMG_WIDTH),
        batch_size=batch_size
    )
    
    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)

    class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))
    # print(class_names)
    
    normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)

    normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
    normalized_val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))

    model = get_compiled_model(class_names)

    history = model.fit(normalized_train_ds,
        epochs=3,
        batch_size=batch_size,
        validation_data=normalized_val_ds
    )

    model.save('model/my_model')
    
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    epochs_range = range(3)
    
    plt.figure(figsize=(8,8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')
    
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()
    
