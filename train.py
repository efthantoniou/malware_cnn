from datetime import datetime
import tensorflow as tf
import pathlib
import numpy as np
import os
from tensorflow.keras import layers
import argparse
import sys


print(tf.__version__)
tf.compat.v1.enable_eager_execution()
#tf.config.optimizer.set_jit(True)
normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)

IMG_HEIGHT = IMG_WIDTH = 256
AUTOTUNE = tf.data.experimental.AUTOTUNE

batch_size = 32

device_name = tf.test.gpu_device_name()
if not device_name:
    raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

def configure_for_performance(ds):
    ds = ds.cache()
    ds = ds.shuffle(buffer_size=1000)
    ds = ds.batch(batch_size)
    ds = ds.prefetch(buffer_size=AUTOTUNE)
    return ds

def get_label(file_path):
    parts = tf.strings.split(file_path, os.path.sep)
    one_hot = parts[-2] == class_names
    return tf.argmax(one_hot)

def decode_img(img):
    img = tf.image.decode_png(img, channels=3)
    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])

def process_path(file_path):
  label = get_label(file_path)
  # load the raw data from the file as a string
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label




if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('-s', '--source', type=str, help='Path to data')
    args = parser.parse_args()

    if args.source:
        pass
    else:
        sys.exit(0)

    data_dir = args.source
    data_dir = pathlib.Path(data_dir)

    image_count = len(list(data_dir.glob('*/*.png')))
    print(image_count)

    list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'), shuffle=False)
    list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)

    class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))
    # print(class_names)

    val_size = int(image_count * 0.3)
    train_ds = list_ds.skip(val_size)
    val_ds = list_ds.take(val_size)

    print("Training dataset: {}".format(tf.data.experimental.cardinality(train_ds).numpy()))
    print("Validation dataset: {}".format(tf.data.experimental.cardinality(val_ds).numpy()))

    #train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
    #val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))

    train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)
    val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)

    train_ds = configure_for_performance(train_ds)
    val_ds = configure_for_performance(val_ds)

    model = tf.keras.Sequential([
        layers.experimental.preprocessing.Rescaling(1./255),
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2,2)),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(len(class_names), activation='softmax')
    ])

    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer='adam', metrics=['accuracy'])

    model.fit(train_ds, epochs=3, steps_per_epoch=350, batch_size=32)

    print("Evaluate")
    result = model.evaluate(val_ds)
    print(dict(zip(model.metrics_names, result)))
    #model.save("my_model")
